{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1: Statistical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we look at the correlations looked at in the previous sectiona nd try to test their statistical significance. We will also capture new correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def str2list(text):\n",
    "    # print(text)\n",
    "    init_ls = text.strip('[]').split(',')\n",
    "    #print (init_ls[1])\n",
    "    final_ls = [X.strip(' ').strip('\\'') for X in init_ls]\n",
    "    # print(final_ls[1])\n",
    "    tag_ls = ['tag_'+X for X in final_ls]\n",
    "    return tag_ls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the output of Data Storytelling\n",
    "df_clean = pd.read_csv(r'After_Storytelling.csv', index_col = 0)\n",
    "df_clean.index.name = \"index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('Shape is', df_clean.shape)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.tags = df_clean.tags.apply(str2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_all = df_clean.tags\n",
    "tag_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls_tags in tags_all:\n",
    "    for tag in ls_tags:\n",
    "        if tag in tag_dict:\n",
    "            tag_dict[tag] = tag_dict[tag]+1\n",
    "        else:\n",
    "            tag_dict[tag] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('We have %d unique tags assigned to all the talks'%len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(tag_dict).sort_values(ascending = False)[0:10].plot.bar(title = 'Most Common Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will try to see whether a particular tag is indicative of different ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the three ratings of interest and drop the rest\n",
    "# Keep the following ratings : Inspiring, Informative and Funny\n",
    "df_clean.drop(['Beautiful', 'Ingenious', 'Courageous', 'Confusing', \n",
    "       'Fascinating', 'Unconvincing', 'OK'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Binary columns for top tags\n",
    "df_clean['tag_technology'] = ['tag_technology' in ls for ls in df_clean.tags]\n",
    "df_clean['tag_science'] = ['tag_science' in ls for ls in df_clean.tags]\n",
    "df_clean['tag_global issues'] = ['tag_global issues' in ls for ls in df_clean.tags]\n",
    "df_clean['tag_culture'] = ['tag_culture' in ls for ls in df_clean.tags]\n",
    "df_clean['tag_design'] = ['tag_design' in ls for ls in df_clean.tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to see if talks with each of the tags score differently compared to talks without the tag for each of our rating of interest. In other words, we would like to ask the following questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 : Are talks with tag 'technology' score differently compared to talks without this tag on the rating 'Informative'? \n",
    "\n",
    "Null Hypotheis : Talks with and without 'technology' tag score similar on 'Informative\n",
    "\n",
    "Alt Hypotheis : Two groups are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = df_clean[df_clean['tag_technology'] == True]\n",
    "non_tech = df_clean[df_clean['tag_technology'] != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean rating for each is\n",
    "print ('Talks with technology tag have average \\'Informative\\' rating of', tech.Informative.mean())\n",
    "print ('Talks without technology tag have average \\'Informative\\' rating of', non_tech.Informative.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a t-test to chcek if the differences are statistically significant\n",
    "from scipy.stats import ttest_ind\n",
    "test_res = ttest_ind(tech.Informative, non_tech.Informative, equal_var = False)\n",
    "\n",
    "test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the test, we see that talks which have 'technology' as tag scored higher on 'Informative' ratings compared to those without this. In other words, we reject null hypotheis. \n",
    "Lets similarly ask other questions\n",
    "\n",
    "Q2 : Do talks with tag 'design' score differently on 'inspiring' compared to talks without?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = df_clean[df_clean['tag_design'] == True]\n",
    "non_design = df_clean[df_clean['tag_design'] != True]\n",
    "# Calculate mean ratings\n",
    "print ('Talks with design tag have average \\'Inspiring\\' rating of', tech.Inspiring.mean())\n",
    "print ('Talks without design tag have average \\'Inspiring\\' rating of', non_tech.Inspiring.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistical significance\n",
    "test_res = ttest_ind(design.Inspiring, non_design.Inspiring, equal_var = False)\n",
    "test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that talks without design as tag score higher on 'Inspiring' than those with. This is counterintuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets drop the tags column and get ready for applying machine learning to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.drop(['tags'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "df_clean.to_csv('After_StatisticalAnalysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
